{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/tapankheni/Data_Science/Data Science Projects/Wafers_Fault_Prediction'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    cols_to_drop: list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from WafersFault.constants import CONFIG_YAML_FILE_PATH, PARAMS_YAML_FILE_PATH, SCHEMA_YAML_FILE_PATH\n",
    "from WafersFault.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self,\n",
    "                 config_path = CONFIG_YAML_FILE_PATH,\n",
    "                 params_path = PARAMS_YAML_FILE_PATH,\n",
    "                 schema_path = SCHEMA_YAML_FILE_PATH):\n",
    "        \n",
    "        self.config = read_yaml(config_path)\n",
    "        self.params = read_yaml(params_path)\n",
    "        self.schema = read_yaml(schema_path)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "        drop_schema = self.schema.COLS_TO_DROP\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path,\n",
    "            cols_to_drop=list(drop_schema.keys()),\n",
    "        )\n",
    "\n",
    "        return data_transformation_config     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from WafersFault import logger\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig) -> None:\n",
    "        self.config = config\n",
    "\n",
    "    def do_data_transformation(self):\n",
    "\n",
    "        df = pd.read_csv(self.config.data_path)\n",
    "\n",
    "        logger.info(\"dropping the unnecessary columns\")\n",
    "        df.drop(columns=self.config.cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "        logger.info(\"spliting dependent and independent features\")\n",
    "        X = df.iloc[:,:-1]\n",
    "        y = df[\"Good/Bad\"]\n",
    "\n",
    "        logger.info(f\"Shape of X: {X.shape}, shape of y: {y.shape}\")\n",
    "\n",
    "        logger.info(\"data preprocessing started....\")\n",
    "        numerical_columns = list(X.columns)\n",
    "\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                (\"Numeric_1\", KNNImputer(n_neighbors=5), numerical_columns),\n",
    "                (\"Numeric_2\", Pipeline([\n",
    "                    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "                    (\"scaler\", (RobustScaler()))\n",
    "                ]), numerical_columns)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        preprocessor_pipeline = Pipeline(\n",
    "            steps=[\n",
    "                (\"preprocessor\", preprocessor)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        X_transformed = preprocessor_pipeline.fit_transform(X)\n",
    "        logger.info(f\"Shape of the transformed X: {X.shape}\")\n",
    "        logger.info(\"data preprocessing completed....\")\n",
    "\n",
    "        logger.info(\"class imbalance handling phase started\")\n",
    "\n",
    "        resampler = SMOTETomek(sampling_strategy=\"auto\")\n",
    "        X_res, y_res = resampler.fit_resample(X_transformed, y)\n",
    "\n",
    "        logger.info(f\"Before resampling, Shape of the training instances: {np.c_[X_transformed, y].shape}\\n\")\n",
    "        logger.info(f\"After resampling, Shape of the training instances: {np.c_[X_res, y_res].shape}\\n\")\n",
    "        logger.info(y.value_counts())\n",
    "        logger.info(y_res.value_counts())\n",
    "\n",
    "        logger.info(\"done with class imbalance\")\n",
    "\n",
    "        logger.info(\"train_test_split started....\")\n",
    "        X_train, y_train, X_test, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42)\n",
    "        y_train = np.where(y_train==-1, 0, 1)\n",
    "        y_test = np.where(y_test==-1, 0, 1)\n",
    "\n",
    "        logger.info(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "        logger.info(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "        logger.info(\"train_test_split completed.....\")\n",
    "\n",
    "        np.save(os.path.join(self.config.root_dir, \"X_train.npy\"), X_train)\n",
    "        np.save(os.path.join(self.config.root_dir, \"X_test.npy\"), X_test)\n",
    "        np.save(os.path.join(self.config.root_dir, \"y_train.npy\"), y_train)\n",
    "        np.save(os.path.join(self.config.root_dir, \"y_test.npy\"), y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-12 11:27:24,425: INFO: common: yaml file: config/config.yaml loaded successfully]\n",
      "[2024-03-12 11:27:24,427: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-03-12 11:27:24,467: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2024-03-12 11:27:24,472: INFO: common: created directory at: artifacts]\n",
      "[2024-03-12 11:27:24,473: INFO: common: created directory at: artifacts/data_transformation]\n",
      "[2024-03-12 11:27:24,483: INFO: 3373966542: dropping the unnecessary columns]\n",
      "[2024-03-12 11:27:24,484: INFO: 3373966542: spliting dependent and independent features]\n",
      "[2024-03-12 11:27:24,484: INFO: 3373966542: Shape of X: (100, 464), shape of y: (100,)]\n",
      "[2024-03-12 11:27:24,485: INFO: 3373966542: data preprocessing started....]\n",
      "[2024-03-12 11:27:24,550: INFO: 3373966542: Shape of the transformed X: (100, 464)]\n",
      "[2024-03-12 11:27:24,552: INFO: 3373966542: data preprocessing completed....]\n",
      "[2024-03-12 11:27:24,553: INFO: 3373966542: class imbalance handling phase started]\n",
      "[2024-03-12 11:27:24,570: INFO: 3373966542: Before resampling, Shape of the training instances: (100, 929)\n",
      "]\n",
      "[2024-03-12 11:27:24,571: INFO: 3373966542: After resampling, Shape of the training instances: (188, 929)\n",
      "]\n",
      "[2024-03-12 11:27:24,572: INFO: 3373966542: Good/Bad\n",
      "-1.0    94\n",
      " 1.0     6\n",
      "Name: count, dtype: int64]\n",
      "[2024-03-12 11:27:24,574: INFO: 3373966542: Good/Bad\n",
      "-1.0    94\n",
      " 1.0    94\n",
      "Name: count, dtype: int64]\n",
      "[2024-03-12 11:27:24,575: INFO: 3373966542: done with class imbalance]\n",
      "[2024-03-12 11:27:24,575: INFO: 3373966542: train_test_split started....]\n",
      "[2024-03-12 11:27:24,577: INFO: 3373966542: X_train shape: (150, 928), y_train shape: (38, 928)]\n",
      "[2024-03-12 11:27:24,583: INFO: 3373966542: X_test shape: (150,), y_test shape: (38,)]\n",
      "[2024-03-12 11:27:24,588: INFO: 3373966542: train_test_split completed.....]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    obj = DataTransformation(config=data_transformation_config)\n",
    "    obj.do_data_transformation()\n",
    "\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "waferspred",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
